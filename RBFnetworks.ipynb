{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def clustering(num_clusters, data,data_labels):\n",
    "    mu = {}\n",
    "    sigma = {}\n",
    "    kmeans = KMeans(n_clusters=num_clusters, random_state=0).fit(data)\n",
    "    l = [data_labels[np.where(kmeans.labels_ == i)] for i in range(num_clusters\\\n",
    ")]\n",
    "\n",
    "    #pprint.pprint(l)\n",
    "    #sigma_0 = {i: np.std(data[np.where(kmeans.labels_ == i)], axis=0) for i in\n",
    "     #          range(num_clusters)}\n",
    "    for l in range(0, num_clusters):\n",
    "        mu['cluster_' + str(l)] = kmeans.cluster_centers_[\n",
    "            l] \n",
    "    \n",
    "    sigma = 10.\n",
    "\n",
    "    return mu, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_selection(num_clusters, data,data_labels):\n",
    "    rand_idx = np.random.randint( 0, data.shape[0]-1, size=num_clusters)\n",
    "    mu = {}\n",
    "    for l in range(0, num_clusters):\n",
    "        var = data[rand_idx[l]] \n",
    "        mu['cluster_' + str(l)] =  var#tf.Variable(var,dtype=tf.float32,trainable=True)\n",
    "    sigma = .01\n",
    "   \n",
    "    return mu,sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "#from networks.dense_net import DenseNet\n",
    "from six.moves import xrange\n",
    "import pprint\n",
    "from scipy import special\n",
    "import tensorflow_probability as tfp\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def Train_and_Test_RBF(X_train,X_test,y_train,y_test,Y_train,mu,sigma,num_classes):\n",
    "        '''\n",
    "        this method will define the network and train the model\n",
    "        ''' \n",
    "        batch_size = 100# X_test.shape[0]\n",
    "        \n",
    "        tfd = tfp.distributions\n",
    "        K = 10\n",
    "        runs = 30\n",
    "        tf.compat.v1.disable_eager_execution()\n",
    "        #mu, sigma = random_selection(K,X_train,Y_train)\n",
    "        \n",
    "        x = tf.compat.v1.placeholder(tf.float32, shape=[None, X_train.shape[1]])\n",
    "        y = tf.compat.v1.placeholder(tf.float32, shape=[None, y_test.shape[1]])\n",
    "        #sqrt((r/self.epsilon)**2 + 1)\n",
    "        m_list = list()\n",
    "        for k in range(K):\n",
    "            key = 'cluster_'+str(k)\n",
    "            # this calculates the single point activation as in my paper\n",
    "            v =  tf.pow(tf.square(x-mu[key])/(tf.square(sigma)+1),.5) # tf.pow(tf.reduce_sum(tf.square(x-mu[key],axis=1)/(tf.square(sigma)+1),.5)\n",
    "            m_list.append(v)\n",
    "        #updated_x = tf.transpose(m_list) # Incase the sum is reduced (not  single-point activation)\n",
    "        \n",
    "        updated_x = tf.reduce_mean(m_list,axis=0)\n",
    "        \n",
    "    \n",
    "\n",
    "        output_layer = tf.keras.Sequential([\n",
    "                    layers.Dense(400, activation=tf.nn.sigmoid,name=\"e_1\"),\n",
    "                    layers.Dense(250, activation=tf.nn.sigmoid,name=\"e_2\"),\n",
    "                    layers.Dense(num_classes, activation=tf.nn.sigmoid,name=\"e_3\")      \n",
    "            \n",
    "\n",
    "                ])\n",
    "       \n",
    "    \n",
    "        output_result = output_layer(updated_x)\n",
    "\n",
    "        \n",
    "        cost = tf.reduce_mean(tf.reduce_sum(tf.square(y- output_result),axis=1))#tf.nn.softmax_cross_entropy_with_logits(y, output_result))\n",
    "        optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=0.0001).minimize(cost)\n",
    "        \n",
    "\n",
    "        n_minibatches = int(X_train.shape[0] / batch_size)\n",
    "        print(\"Number of minibatches: \", n_minibatches)\n",
    "\n",
    "        sess = tf.compat.v1.InteractiveSession()\n",
    "        init = tf.compat.v1.global_variables_initializer()\n",
    "        sess.run(init)\n",
    "        for epoch in range(runs):\n",
    "                    pbar = tf.keras.utils.Progbar(n_minibatches)\n",
    "                    for i in range(n_minibatches):\n",
    "                        x_batch = X_train[i * batch_size:(i + 1) * batch_size] \n",
    "                        y_batch = y_train[i * batch_size:(i + 1) * batch_size] \n",
    "                        cost_, _,output_print,updated_print = sess.run((cost, optimizer, output_result,v), feed_dict={x: x_batch,y:y_batch})\n",
    "                    if epoch%5 == 0: \n",
    "                        \n",
    "                        pbar.add(1, [(\"cost\", cost_),( \"epoch\",epoch)])\n",
    "                        print(np.max(updated_print))\n",
    "                        \n",
    "        \n",
    "        z_test =  sess.run(output_result, feed_dict={x: X_test,y:y_test})\n",
    "        \n",
    "        sess.close()\n",
    "        return z_test\n",
    "                        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import preprocessing\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "data = pd.read_csv(\"data/lung.csv\")\n",
    "print(\"data shape before split :\", data.shape)\n",
    "\n",
    "#...................................................................................... Normalization\n",
    "data_normalized = preprocessing.scale(data.values[:,:-1])\n",
    "y = data.values[:,-1] \n",
    "_,num_classes = np.unique(y,return_index = True)\n",
    "print(num_classes)\n",
    "data_normalized = np.hstack((data_normalized,y[:,np.newaxis]))\n",
    "\n",
    "\n",
    "\n",
    "#.....................................................................................splitting into 10-folds\n",
    "kf = KFold(n_splits=10,shuffle=True) # Define the split - into 10 folds \n",
    "kf.get_n_splits(data_normalized) # returns the number of splitting iterations in the cross-validator\n",
    "#print(kf) \n",
    "\n",
    "#.....................................................................................start_training\n",
    "RF_score_cumm,SMO_score_cumm,NB_score_cumm,LR_score_cumm = 0.,0.,0.,0.\n",
    "VAE_RF_score_cumm,VAE_SMO_score_cumm,VAE_NB_score_cumm,VAE_LR_score_cumm = 0.,0.,0.,0.\n",
    "GMV_RF_score_cumm,GMV_SMO_score_cumm,GMV_NB_score_cumm,GMV_LR_score_cumm = 0.,0.,0.,0.\n",
    "\n",
    "K = 30 # number of clusters\n",
    "for train_index, test_index in kf.split(data_normalized):\n",
    "     print(\"########################## fold #################\")\n",
    "     #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "     X_train, X_test = data_normalized[train_index,:-1], data_normalized[test_index,:-1]\n",
    "     y_train, y_test = data_normalized[train_index,-1], data_normalized[test_index,-1]\n",
    "    \n",
    "     enc = OneHotEncoder(handle_unknown='ignore')\n",
    "     enc.fit(y_train[:,np.newaxis])\n",
    "     Y_train = enc.transform(y_train[:,np.newaxis]).toarray()\n",
    "  \n",
    "\n",
    "     Y_test = enc.transform(y_test[:,np.newaxis]).toarray()\n",
    "    \n",
    "     mu, sigma = random_selection(K,X_train,Y_train)\n",
    "     \n",
    "     # call the NN-model\n",
    "     test_class = Train_and_Test_RBF(X_train,X_test,Y_train,Y_test,y_train,mu,sigma,num_classes.shape[0])\n",
    "     print(test_class)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import preprocessing\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn import metrics\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "print(\" Y train shape : \", y_train)\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "     \n",
    "enc.fit(y_train[:,np.newaxis])\n",
    "Y_train = enc.transform(y_train[:,np.newaxis]).toarray()\n",
    "#print(Y_train[0:10,:])\n",
    "\n",
    "\n",
    "Y_test = enc.transform(y_test[:,np.newaxis]).toarray()\n",
    "     \n",
    "     # call the NN-model\n",
    "mu, sigma = random_selection(30,X_train.reshape((X_train.shape[0],28*28))/255.,Y_train)\n",
    "test_class = Train_and_Test_RBF(X_train.reshape((X_train.shape[0],28*28))/255., \n",
    "                                X_test.reshape((X_test.shape[0],28*28))/255.,\n",
    "                                Y_train, Y_test,y_train,mu,sigma,10)\n",
    "#print(np.argmax(test_class,axis=1)[0:20])\n",
    "#print(np.argmax(Y_test,axis=1)[0:20])\n",
    "f_report = metrics.accuracy_score(np.argmax(Y_test,axis=1),np.argmax(test_class,axis=1), normalize=True)\n",
    "print(\"f-score : \", f_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
