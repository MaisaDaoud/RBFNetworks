{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def clustering(num_clusters, data,data_labels):\n",
    "    mu = {}\n",
    "    sigma = {}\n",
    "    kmeans = KMeans(n_clusters=num_clusters, random_state=0).fit(data)\n",
    "    l = [data_labels[np.where(kmeans.labels_ == i)] for i in range(num_clusters\\\n",
    ")]\n",
    "\n",
    "    #pprint.pprint(l)\n",
    "    sigma_0 = {i: np.std(data[np.where(kmeans.labels_ == i)], axis=0) for i in\n",
    "               range(num_clusters)}\n",
    "    for l in range(0, num_clusters):\n",
    "        mu['cluster_' + str(l)] = kmeans.cluster_centers_[\n",
    "            l] \n",
    "    for l in range(0, num_clusters):\n",
    "        sigma['cluster_' + str(l)] = sigma_0[l] \n",
    "\n",
    "    return mu, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "#from networks.dense_net import DenseNet\n",
    "from six.moves import xrange\n",
    "import pprint\n",
    "from scipy import special\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "\n",
    "def Train_and_Test_RBF(X_train,X_test,y_train,y_test,cancer_size,minority_idx):\n",
    "            \n",
    "\n",
    "       \n",
    "        normal_size = len(X_train) - cancer_size\n",
    "        print('len(data_samples) ',len(X_train))\n",
    "        print('cancer_size ' , cancer_size)\n",
    "        print('normal_size ' , normal_size)\n",
    " \n",
    "        lables  = [] # np.zeros([replication, rep_length]) \n",
    "       \n",
    "        # this method will define the network and train the model\n",
    "        batch_size = 25 # X_test.shape[0]\n",
    "        z_dim = 500\n",
    "        tfd = tfp.distributions\n",
    "        K = 5\n",
    "        runs = 300\n",
    "        mu, sigma = clustering(K,X_train,y_train)\n",
    "        \n",
    "\n",
    "        encoder = tf.keras.Sequential([\n",
    "                    layers.Dense(1000, activation=tf.nn.sigmoid,name=\"e_1\")\n",
    "                    \n",
    "            \n",
    "\n",
    "                ])\n",
    "\n",
    "        decoder = {}\n",
    "        for k in range(K):\n",
    "            key = 'cluster_'+str(k)\n",
    "            decoder[key] =  tf.keras.Sequential([\n",
    "                  \n",
    "                  layers.Dense(1000, activation=tf.nn.sigmoid,name=\"d1\")\n",
    "                ])\n",
    "\n",
    "\n",
    "        tf.compat.v1.disable_eager_execution()\n",
    "        x = tf.compat.v1.placeholder(tf.float32, shape=[None, X_train.shape[1]])\n",
    "        #encoded = encoder(x)\n",
    "\n",
    "        mean = layers.Dense(z_dim, tf.nn.sigmoid,name=\"mean_latent\")(x) #encoded\n",
    "        sigma = layers.Dense(z_dim, tf.nn.sigmoid,name=\"var_latent\")(x) #encoded\n",
    "\n",
    "        z = mean + tf.multiply(tf.sqrt(tf.exp(sigma)),\n",
    "                                       tf.random.normal(shape=(tf.shape(mean)[0], z_dim)))\n",
    "        x_reco = {}\n",
    "        mean_decoder = {}\n",
    "        sigma_decoder = {}\n",
    "        rec_loss_per_cluster = {}\n",
    "        for k in range(K):\n",
    "            key = 'cluster_'+str(k)\n",
    "            x_reco[key] = decoder[key](z)\n",
    "            mean_decoder[key] = layers.Dense(X_train.shape[1], tf.nn.sigmoid,name=\"mean_decoder\")(z) #x_reco[key]\n",
    "            sigma_decoder[key] = layers.Dense(X_train.shape[1], tf.nn.sigmoid,name=\"var_decoder\")(z)#x_reco[key]\n",
    "               \n",
    "            rec_loss_per_cluster[key] = tf.reduce_sum(tf.square(mean_decoder[key] - mu[key]),axis=1)#+\\\n",
    "            #tf.reduce_sum(sigma_decoder[key][0] - sigma[key],axis=1)\n",
    "       \n",
    "        \n",
    "        reconstruction_term =  tf.reduce_mean(list(rec_loss_per_cluster.values()) )\n",
    "\n",
    "        mvn = tfd.MultivariateNormalDiag(loc=[0. for i in range(z_dim)],scale_diag=[1. for i in range(z_dim)])\n",
    "        #kl_divergence = tf.reduce_sum(tf.keras.metrics.kullback_leibler_divergence(z,mvn.sample(z.shape[0])))#self.mu_d[key]/255., x_reco[key]), axis=[1,2])\n",
    "\n",
    "        cost = tf.reduce_mean(reconstruction_term) #+ tf.reduce_mean(kl_divergence)\n",
    "        optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=0.001).minimize(cost)\n",
    "        \n",
    "\n",
    "        n_minibatches = int(X_train.shape[0] / batch_size)\n",
    "        if(int(X_train.shape[0] % batch_size)) >0:\n",
    "            n_minibatches +=1\n",
    "        print(\"Number of minibatches: \", n_minibatches)\n",
    "\n",
    "        sess = tf.compat.v1.InteractiveSession()\n",
    "        init = tf.compat.v1.global_variables_initializer()\n",
    "        sess.run(init)\n",
    "        for epoch in range(runs):\n",
    "                    pbar = tf.keras.utils.Progbar(n_minibatches)\n",
    "                    for i in range(n_minibatches):\n",
    "                        x_batch = X_train[i * batch_size:(i + 1) * batch_size] \n",
    "                        cost_, _,z_print = sess.run((cost, optimizer, z), feed_dict={x: x_batch})\n",
    "                        if epoch%100 == 0:\n",
    "                            pbar.add(1, [(\"cost\", cost_),( \"epoch\",epoch)])\n",
    "                        \n",
    "        train_z_test = mean + tf.multiply(tf.sqrt(tf.exp(sigma)),\n",
    "                                       tf.random.normal(shape=(X_train.shape[0], z_dim)))\n",
    "        test_z_test = mean + tf.multiply(tf.sqrt(tf.exp(sigma)),\n",
    "                                       tf.random.normal(shape=(X_test.shape[0], z_dim)))\n",
    "      \n",
    "        z_train = sess.run(train_z_test, feed_dict={x: X_train})\n",
    "        z_test =  sess.run(test_z_test, feed_dict={x: X_test})\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        sess.close()\n",
    "        return z_train,z_test,y_train\n",
    "                        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "#from networks.dense_net import DenseNet\n",
    "from six.moves import xrange\n",
    "import pprint\n",
    "from scipy import special\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "\n",
    "def Train_and_Test_VAE(X_train,X_test,y_train,y_test):\n",
    "        # this method will define the network and train the model\n",
    "        batch_size = 25 # X_test.shape[0]\n",
    "        z_dim = 250\n",
    "        tfd = tfp.distributions\n",
    "        \n",
    "        runs = 300\n",
    "        \n",
    "\n",
    "        encoder = tf.keras.Sequential([\n",
    "                    layers.Dense(500, activation=tf.nn.sigmoid,name=\"e_1\")\n",
    "                    \n",
    "                   \n",
    "\n",
    "                ])\n",
    "\n",
    "        \n",
    "        decoder =  tf.keras.Sequential([\n",
    "        \n",
    "                  layers.Dense(500, activation=tf.nn.sigmoid,name=\"d_2\")\n",
    "                 \n",
    "                ])\n",
    "\n",
    "\n",
    "        tf.compat.v1.disable_eager_execution()\n",
    "        x = tf.compat.v1.placeholder(tf.float32, shape=[None, X_train.shape[1]])\n",
    "        encoded = encoder(x)\n",
    "\n",
    "        mean = layers.Dense(z_dim, tf.nn.sigmoid,name=\"mean_latent\")(x)#encoded)\n",
    "        sigma = layers.Dense(z_dim, tf.nn.sigmoid,name=\"var_latent\")(x)#encoded)\n",
    "\n",
    "        z = mean + tf.multiply(tf.sqrt(tf.exp(sigma)),\n",
    "                                       tf.random.normal(shape=(batch_size, z_dim)))\n",
    "        \n",
    "        x_reco = decoder(z)\n",
    "        mean_decoder = layers.Dense(X_train.shape[1], tf.nn.sigmoid,name=\"mean_decoder\")(z)#x_reco)\n",
    "        sigma_decoder = layers.Dense(X_train.shape[1], tf.nn.sigmoid,name=\"var_decoder\")(z)#x_reco)\n",
    "               \n",
    "        rec_loss_per_cluster = tf.reduce_sum(tf.square(mean_decoder- x),axis=1)#+\\\n",
    "            #tf.reduce_sum(sigma_decoder[key][0] - sigma[key],axis=1)\n",
    "       \n",
    "        \n",
    "        reconstruction_term =  tf.reduce_mean(rec_loss_per_cluster )\n",
    "\n",
    "        mvn = tfd.MultivariateNormalDiag(loc=[0. for i in range(z_dim)],scale_diag=[1. for i in range(z_dim)])\n",
    "        kl_divergence = tf.reduce_sum(tf.keras.metrics.kullback_leibler_divergence(z,mvn.sample(z.shape[0])))#self.mu_d[key]/255., x_reco[key]), axis=[1,2])\n",
    "\n",
    "        cost = tf.reduce_mean(reconstruction_term) + tf.reduce_mean(kl_divergence)\n",
    "        optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=0.0001).minimize(cost)\n",
    "        \n",
    "\n",
    "        n_minibatches = int(X_train.shape[0] / batch_size)\n",
    "        print(\"Number of minibatches: \", n_minibatches)\n",
    "\n",
    "        sess = tf.compat.v1.InteractiveSession()\n",
    "        init = tf.compat.v1.global_variables_initializer()\n",
    "        sess.run(init)\n",
    "        for epoch in range(runs):\n",
    "                    pbar = tf.keras.utils.Progbar(n_minibatches)\n",
    "                    for i in range(n_minibatches):\n",
    "                        x_batch = X_train[i * batch_size:(i + 1) * batch_size] \n",
    "                        cost_, _,z_print = sess.run((cost, optimizer, z), feed_dict={x: x_batch})\n",
    "                        pbar.add(1, [(\"cost\", cost_),( \"epoch\",epoch)])\n",
    "                        \n",
    "        train_z_test = mean + tf.multiply(tf.sqrt(tf.exp(sigma)),\n",
    "                                       tf.random.normal(shape=(X_train.shape[0], z_dim)))\n",
    "        test_z_test = mean + tf.multiply(tf.sqrt(tf.exp(sigma)),\n",
    "                                       tf.random.normal(shape=(X_test.shape[0], z_dim)))\n",
    "        x_reco = {}\n",
    "        z_train = sess.run(train_z_test, feed_dict={x: X_train})\n",
    "        z_test =  sess.run(test_z_test, feed_dict={x: X_test})\n",
    "        \n",
    "        sess.close()\n",
    "        return z_train,z_test\n",
    "                        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "#from networks.dense_net import DenseNet\n",
    "from six.moves import xrange\n",
    "import pprint\n",
    "from scipy import special\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "\n",
    "def Train_and_Test_GMVAE(X_train,X_test,y_train,y_test):\n",
    "        # this method will define the network and train the model\n",
    "        batch_size = 25 # X_test.shape[0]\n",
    "        hidden_dim = 500\n",
    "        z_dim = 250\n",
    "        tfd = tfp.distributions\n",
    "        K = 5\n",
    "        w_dim  = 50\n",
    "        \n",
    "        runs =300\n",
    "        tf.compat.v1.disable_eager_execution()\n",
    "        y = tf.compat.v1.placeholder(tf.float32, shape=[None, X_train.shape[1]])\n",
    "        \n",
    "        ########################## ENCODER PART ###################################\n",
    "        # The representtions X\n",
    "        \n",
    "        Qx_y= tf.keras.Sequential([\n",
    "                    layers.Dense(hidden_dim, activation=tf.nn.sigmoid,name=\"Qy_x_1\")\n",
    "\n",
    "                ]) \n",
    "        Qx_y_mean = layers.Dense(z_dim, tf.nn.sigmoid,name=\"Qx_y_mean\")(Qx_y(y))\n",
    "        Qx_y_var= layers.Dense(z_dim, tf.nn.sigmoid,name=\"Qx_y_var\")(Qx_y(y))\n",
    "\n",
    "        Qx_y_samples= Qx_y_mean + tf.multiply(tf.sqrt(tf.exp(Qx_y_var)),\n",
    "                                       tf.random.normal(shape=(batch_size, z_dim)))\n",
    "        \n",
    "        #................................................................................\n",
    "        \n",
    "        \n",
    "        Qw_y =   tf.keras.Sequential([\n",
    "                    layers.Dense(hidden_dim, activation=tf.nn.sigmoid,name=\"Qw_y_1\")\n",
    "                \n",
    "                ])\n",
    "        Qw_y_mean = layers.Dense(w_dim, tf.nn.sigmoid,name=\"Qw_y_mean\")(Qw_y(y))\n",
    "        Qw_y_var  = layers.Dense(w_dim, tf.nn.sigmoid,name=\"Qw_y_var\")(Qw_y(y))\n",
    "         \n",
    "                                                    \n",
    "        Qw_y_samples= Qw_y_mean + tf.multiply(tf.sqrt(tf.exp(Qw_y_var)),\n",
    "                                       tf.random.normal(shape=(batch_size, w_dim)))    \n",
    "            \n",
    "        \n",
    "        #...............................................................................\n",
    "        # The components coeff Z\n",
    "        xw = tf.concat([Qw_y_samples, Qx_y_samples],1, name='xw_concat')\n",
    "        Qz_xw = tf.keras.Sequential([\n",
    "                    layers.Dense(z_dim, activation=tf.nn.sigmoid,name=\"Qxw_z_1\")\n",
    "                ])\n",
    "        Qz_xw_mean = layers.Dense(K, tf.nn.sigmoid,name=\"Qz_xw_mean\")(Qz_xw(xw))\n",
    "        Qz_xw_var= layers.Dense(K, tf.nn.sigmoid,name=\"Qz_xw_var\")(Qz_xw(xw))\n",
    "\n",
    "        Qz_samples= Qz_xw_mean + tf.multiply(tf.sqrt(tf.exp(Qz_xw_var)),\n",
    "                                       tf.random.normal(shape=(batch_size, K)))\n",
    "        \n",
    "        \n",
    "        ###################################\n",
    "        ########################################\n",
    "        ####################### Decoder ###########################################\n",
    "        \n",
    "        # Px_wz, https://arxiv.org/pdf/1611.02648.pdf\n",
    "        #More specifically, the neural network parameterised by β outputs a set of K means µzk and K variances σ**2 zk\n",
    "        #, given w as input. A one-hot vector z is sampled from the mixing probability π, which chooses one component \n",
    "        #from the Gaussian mixture. We set the parameter πk = K**−1 to make z uniformly\n",
    "        #distributed. The generative and variational views of this model are depicte\n",
    "        #......................Px_wz_mean .....................\n",
    "        #wz = tf.concat([Qw_samples, Qz_samples],1, name='wz_concat')\n",
    "        Px_wz_mean_dict = {}\n",
    "        for k in range(K): \n",
    "            key = 'cluster_'+str(k)\n",
    "            \n",
    "            Px_wz_mean = layers.Dense(z_dim, tf.nn.sigmoid,name=\"Py_x_mean\")(Qw_y_samples)\n",
    "            Px_wz_mean_dict[key] = Px_wz_mean\n",
    "        #........................... Px_wz_var ......................  \n",
    "        Px_wz_var_dict = {}\n",
    "        for k in range(K):                                    \n",
    "            key = 'cluster_'+str(k)\n",
    "            Px_wz_var = layers.Dense(z_dim, tf.nn.sigmoid,name=\"Py_x_mean\")(Qw_y_samples)\n",
    "            Px_wz_var_dict[key] = Px_wz_var\n",
    "        \n",
    "         #.................. sampling Px_wz ...........\n",
    "        # Sample Z as one-hot categioral \n",
    "        temperature = 1e-5\n",
    "        z_dist = tfd.RelaxedOneHotCategorical(temperature, probs=Qz_samples)\n",
    "        z_samples = z_dist.sample(1)\n",
    "                                            \n",
    "        #......... Define the GMM to sample from \n",
    "        gm = tfd.MixtureSameFamily(\n",
    "        mixture_distribution= tfd.Categorical(     \n",
    "        probs=Qz_samples),\n",
    "            components_distribution= tfd.MultivariateNormalDiag(\n",
    "                loc=[[Px_wz_mean_dict['cluster_'+str(k)][i] for k in range(K)]for i in range(batch_size) ],\n",
    "                scale_diag=[[tf.sqrt(tf.exp(Px_wz_var_dict['cluster_'+str(k)][i])) for k in range(K)]for i in range(batch_size) ],name=\"prior_code\"))\n",
    "        \n",
    "        Px_wz_samples = gm.sample(1)\n",
    "                                            \n",
    "        #.......................Py_x ..............................\n",
    "        Py_x =   tf.keras.Sequential([\n",
    "                    layers.Dense(hidden_dim, activation=tf.nn.sigmoid,name=\"Qw_y_1\")\n",
    "                \n",
    "                ])\n",
    "        Py_x_mean = layers.Dense(X_train.shape[1], tf.nn.sigmoid,name=\"Py_x_mean\")(Py_x(Px_wz_samples))\n",
    "        Py_x_var  = layers.Dense(X_train.shape[1], tf.nn.sigmoid,name=\"Py_x_var\")(Py_x(Px_wz_samples))\n",
    "         \n",
    "                                                    \n",
    "        Py_x_samples= Py_x_mean + tf.multiply(tf.sqrt(tf.exp(Py_x_var)),\n",
    "                                       tf.random.normal(shape=(batch_size, X_train.shape[1])))\n",
    "        ################################################################################\n",
    "       ####################################################### loss \n",
    "        \n",
    "           # recons_loss, #log p(y|x) \n",
    "        #Py_x_dist =  tfp.distributions.MultivariateNormalDiag(\n",
    "          #             loc=Py_x_mean, \n",
    "         #               scale_diag=Py_x_var,  name='Py_x') \n",
    "           \n",
    "        #logy_x = Py_x_dist.log_prob(X_train)\n",
    "        \n",
    "        y_x_loss = tf.pow(tf.reduce_sum(tf.square(y-Py_x_samples),axis=1),.5)\n",
    "           # q loss KL(q(x|y)||p(x|w, z))\n",
    "        KLxy_xwz = 0.\n",
    "        \n",
    "        #for k in range(K):\n",
    "         #      key = 'cluster_'+str(k)\n",
    "        kl_divergence = tf.reduce_sum(tf.keras.metrics.kullback_leibler_divergence(Qx_y_samples,Px_wz_samples))\n",
    "        '''\n",
    "               KLxy_xwz += tf.reduce_sum((tf.math.log(Qx_y_var) - tf.math.log(Px_wz_var_dict[key]) -\n",
    "            .5 * (1. - (Px_wz_var_dict[key]**2 + (Qx_y_mean - Px_wz_mean_dict[key] )**2) / Qx_y_var**2)),axis=1)\n",
    "           '''\n",
    "           #KL(q(w|y)||p(w))\n",
    "        mvn = tfd.MultivariateNormalDiag(loc=[0. for i in range(w_dim)],scale_diag=[1. for i in range(w_dim)])\n",
    "        KLwy_w = tf.reduce_sum(tf.keras.metrics.kullback_leibler_divergence(Qw_y_samples,mvn.sample(batch_size)))\n",
    "                                              \n",
    "        #KLwy_w =  tf.reduce_sum((tf.math.log(Qw_y_var) - tf.math.log(1.) -\n",
    "         #   .5 * (1. - (Qw_y_var**2 + (Qw_y_mean - 0. )**2) / 1.**2)),axis=1)                                                                    \n",
    "                                                                                 \n",
    "           #KL(p(z|x, w)||p(z))\n",
    "        mvn = tfd.MultivariateNormalDiag(loc=[0. for i in range(K)],scale_diag=[1. for i in range(K)])\n",
    "        KLzxw_z = tf.reduce_sum(tf.keras.metrics.kullback_leibler_divergence(Qz_samples,mvn.sample(batch_size)))\n",
    "                                                                                 \n",
    "        #KLzxw_z = tf.reduce_sum((tf.math.log(Qz_xw_var) - tf.math.log(1.) -\n",
    "         #   .5 * (1. - (Qz_xw_var**2 + (Qz_xw_mean - 0. )**2) / 1.**2)) ,axis=1)\n",
    "        #tf.reduce_mean(logy_x)\n",
    "                                                                          \n",
    "        cost =- tf.reduce_mean(KLxy_xwz) - tf.reduce_mean(KLwy_w) - tf.reduce_mean(KLzxw_z)\n",
    "        optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=0.0001).minimize(cost)\n",
    "                                            \n",
    "            \n",
    "        n_minibatches = int(X_train.shape[0] / batch_size)\n",
    "        print(\"Number of minibatches: \", n_minibatches)\n",
    "\n",
    "        sess = tf.compat.v1.InteractiveSession()\n",
    "        init = tf.compat.v1.global_variables_initializer()\n",
    "        sess.run(init)\n",
    "        for epoch in range(runs):\n",
    "                    pbar = tf.keras.utils.Progbar(n_minibatches)\n",
    "                    for i in range(n_minibatches):\n",
    "                        x_batch = X_train[i * batch_size:(i + 1) * batch_size] \n",
    "                        cost_, _,z_print = sess.run((cost, optimizer, Qx_y_samples), feed_dict={y: x_batch})\n",
    "                        pbar.add(1, [(\"cost\", cost_),( \"epoch\",epoch)])\n",
    "                        \n",
    "        train_z_test = Qx_y_mean + tf.multiply(tf.sqrt(tf.exp(Qx_y_var)),\n",
    "                                       tf.random.normal(shape=(X_train.shape[0], z_dim)))\n",
    "        test_z_test = Qx_y_mean + tf.multiply(tf.sqrt(tf.exp(Qx_y_var)),\n",
    "                                       tf.random.normal(shape=(X_test.shape[0], z_dim)))\n",
    "        x_reco = {}\n",
    "        z_train = sess.run(train_z_test, feed_dict={y: X_train})\n",
    "        z_test =  sess.run(test_z_test, feed_dict={y: X_test})\n",
    "        \n",
    "        sess.close()\n",
    "        return z_train,z_test       \n",
    "        \n",
    "                        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import neighbors, datasets\n",
    "from sklearn import tree\n",
    "from sklearn import svm, datasets,gaussian_process, linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from scipy import interp\n",
    "from sklearn.metrics import precision_score,recall_score,f1_score,accuracy_score,roc_auc_score\n",
    "\n",
    "def classifying_data(train_reps,test_reps,y_train,y_test):\n",
    "     RF_classifier = RandomForestClassifier(n_estimators=100, criterion='gini',\\\n",
    "     random_state=0, max_depth=100,bootstrap=True)\n",
    "     RF_classifier.fit(train_reps,y_train)\n",
    "     y_score = RF_classifier.predict(test_reps)\n",
    "     score_RF = accuracy_score(y_test, y_score, normalize=True,\n",
    "                       sample_weight=None)\n",
    "    \n",
    "     SMO_classifier = svm.SVC( gamma='auto') #'scale'\n",
    "     SMO_classifier.fit(train_reps,y_train)\n",
    "     y_score = SMO_classifier.predict(test_reps)\n",
    "     score_SMO = accuracy_score(y_test, y_score, normalize=True,\n",
    "                       sample_weight=None)\n",
    "    \n",
    "     NB_classifier = GaussianNB(var_smoothing=100)\n",
    "     NB_classifier.fit(train_reps,y_train)\n",
    "     y_score = NB_classifier.predict(test_reps)\n",
    "     score_NB = accuracy_score(y_test, y_score, normalize=True,\n",
    "                       sample_weight=None)\n",
    "    \n",
    "     LR_classifier = linear_model.LogisticRegressionCV(multi_class=\"ovr\",max_iter=500)\n",
    "     LR_classifier.fit(train_reps,y_train)\n",
    "     y_score = LR_classifier.predict(test_reps)\n",
    "     LR_score = accuracy_score(y_test, y_score, normalize=True,\n",
    "                       sample_weight=None)\n",
    "\n",
    "\n",
    "     return score_RF,score_SMO,score_NB,LR_score\n",
    "\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape before split : (186, 19994)\n",
      "data shape after normalization: (186, 19994)\n",
      "########################## fold #################\n",
      "Majority Size :  88\n",
      "len(data_samples)  167\n",
      "cancer_size  88\n",
      "normal_size  79\n",
      "WARNING:tensorflow:From /home/mtdd1/anaconda3/envs/env/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py:1659: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /home/mtdd1/anaconda3/envs/env/lib/python3.8/site-packages/tensorflow/python/ops/linalg/linear_operator_diag.py:159: calling LinearOperator.__init__ (from tensorflow.python.ops.linalg.linear_operator) with graph_parents is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Do not pass `graph_parents`.  They will  no longer be used.\n",
      "Number of minibatches:  7\n",
      "7/7 [==============================] - 1s 208ms/step - cost: 14849.2116 - epoch: 0.0000e+00\n",
      "7/7 [==============================] - 1s 199ms/step - cost: 7877.0691 - epoch: 100.0000\n",
      "7/7 [==============================] - 1s 194ms/step - cost: 7873.9316 - epoch: 200.0000\n",
      "########################## fold #################\n",
      "Majority Size :  84\n",
      "len(data_samples)  168\n",
      "cancer_size  84\n",
      "normal_size  84\n",
      "Number of minibatches:  7\n",
      "7/7 [==============================] - 2s 238ms/step - cost: 14550.8411 - epoch: 0.0000e+00\n",
      "7/7 [==============================] - 1s 200ms/step - cost: 7660.5722 - epoch: 100.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import preprocessing\n",
    "\n",
    "data = pd.read_csv(\"data/SMK_CAN_187.csv\")#,header=None)\n",
    "print(\"data shape before split :\", data.shape)\n",
    "\n",
    "#...................................................................................... Normalization\n",
    "data_normalized = preprocessing.scale(data.values[:,:-1])\n",
    "y = data.values[:,-1] \n",
    "data_normalized = np.hstack((data_normalized,y[:,np.newaxis]))\n",
    "print(\"data shape after normalization:\", data_normalized.shape)\n",
    "\n",
    "\n",
    "#.....................................................................................splitting into 10-folds\n",
    "kf = KFold(n_splits=10,shuffle=True) # Define the split - into 10 folds \n",
    "kf.get_n_splits(data_normalized) # returns the number of splitting iterations in the cross-validator\n",
    "#print(kf) \n",
    "\n",
    "#.....................................................................................start_training\n",
    "RF_score_cumm,SMO_score_cumm,NB_score_cumm,LR_score_cumm = [],[],[],[]\n",
    "VAE_RF_score_cumm,VAE_SMO_score_cumm,VAE_NB_score_cumm,VAE_LR_score_cumm = [],[],[],[]\n",
    "GMV_RF_score_cumm,GMV_SMO_score_cumm,GMV_NB_score_cumm,GMV_LR_score_cumm = [],[],[],[]\n",
    "RF_training_score = []\n",
    "\n",
    "\n",
    "for train_index, test_index in kf.split(data_normalized):\n",
    "     print(\"########################## fold #################\")\n",
    "     #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "     X_train, X_test = data_normalized[train_index,:-1], data_normalized[test_index,:-1]\n",
    "     y_train, y_test = data_normalized[train_index,-1], data_normalized[test_index,-1]\n",
    "     #print(\"TRAIN:\", y_train, \"TEST:\", y_test)\n",
    "     df = pd.DataFrame(y_train, columns=[\"val\"])\n",
    "     splits = df.groupby(\"val\").val.apply(pd.Series.tolist).values\n",
    "\n",
    "     majority_size = 0\n",
    "     minority_idx = 0\n",
    "     minority_size = 10000\n",
    "     for i in range(splits.shape[0]):\n",
    "\n",
    "                if len(splits[i]) > majority_size:\n",
    "                    majority_size = len(splits[i])\n",
    "                if len(splits[i]) < minority_size:\n",
    "                    minority_size = len(splits[i])\n",
    "                    minority_idx = splits[i][0]\n",
    "     print(\"Majority Size : \",majority_size)\n",
    "     \n",
    "     # call the NN-model\n",
    "     train_reps,test_reps,train_lables = Train_and_Test_RBF(X_train,X_test,y_train,y_test,majority_size,minority_idx)\n",
    "     # classify the reps\n",
    "     RF_score,SMO_score,NB_score,LR_score = classifying_data(train_reps,test_reps,train_lables,y_test ) #y_train\n",
    "     #rf_train,SMO_train,NB_train,LR_train =  classifying_data(train_reps,train_reps,y_train,y_train )\n",
    "     print(\"RF = \",RF_score, \"SMO = \", SMO_score,\"NB = \",NB_score,\"LR = \",LR_score)\n",
    "     RF_score_cumm.append(RF_score)\n",
    "     SMO_score_cumm.append(SMO_score )\n",
    "     NB_score_cumm.append(NB_score)\n",
    "     LR_score_cumm.append(LR_score)\n",
    "     #RF_training_score.append(NB_train)\n",
    "     \n",
    "     train_reps,test_reps = Train_and_Test_VAE(X_train,X_test,y_train,y_test)\n",
    "     RF_score,SMO_score,NB_score,LR_score = classifying_data(train_reps,test_reps,y_train,y_test )\n",
    "     print(\"RF = \",RF_score, \"SMO = \", SMO_score,\"NB = \",NB_score,\"LR = \",LR_score)\n",
    "     VAE_RF_score_cumm.append(RF_score)\n",
    "     VAE_SMO_score_cumm.append(SMO_score) \n",
    "     VAE_NB_score_cumm.append(NB_score)\n",
    "     VAE_LR_score_cumm.append(LR_score)\n",
    "     \n",
    "     \n",
    "     train_reps,test_reps = Train_and_Test_GMVAE(X_train,X_test,y_train,y_test)\n",
    "     RF_score,SMO_score,NB_score,LR_score = classifying_data(train_reps,test_reps,y_train,y_test )\n",
    "     print(\"RF = \",RF_score, \"SMO = \", SMO_score,\"NB = \",NB_score,\"LR = \",LR_score)\n",
    "     GMV_RF_score_cumm.append(RF_score)\n",
    "     GMV_SMO_score_cumm.append(SMO_score) \n",
    "     GMV_NB_score_cumm.append(NB_score)\n",
    "     GMV_LR_score_cumm.append(LR_score)\n",
    "     \n",
    "\n",
    "print(\"_________________________Multi___________________________________\")\n",
    "print(\"$\",\"{:.3f}\".format(np.mean(RF_score_cumm)), \" \\pm  \",\n",
    "                           \"{:.3f}\".format(np.std(RF_score_cumm)), \"$ & \",\n",
    "                           \"$\",\"{:.3f}\".format(np.mean(SMO_score_cumm)), \" \\pm  \",\n",
    "                           \"{:.3f}\".format(np.std(SMO_score_cumm)), \"$ & \", \n",
    "                           \"$\",\"{:.3f}\".format(np.mean(NB_score_cumm)), \" \\pm  \",\n",
    "                           \"{:.3f}\".format(np.std(NB_score_cumm)), \"$ & \",\n",
    "                           \"$\",\"{:.3f}\".format(np.mean(LR_score_cumm)), \" \\pm  \",\n",
    "                           \"{:.3f}\".format(np.std(LR_score_cumm)), \"$  \")\n",
    "print(\"_________________________VAE___________________________________\")\n",
    "print(\"$\",\"{:.3f}\".format(np.mean(VAE_RF_score_cumm)), \" \\pm  \",\n",
    "                           \"{:.3f}\".format(np.std(VAE_RF_score_cumm)), \"$ & \",\n",
    "                           \"$\",\"{:.3f}\".format(np.mean(VAE_SMO_score_cumm)), \" \\pm  \",\n",
    "                           \"{:.3f}\".format(np.std(VAE_SMO_score_cumm)), \"$ & \", \n",
    "                           \"$\",\"{:.3f}\".format(np.mean(VAE_NB_score_cumm)), \" \\pm  \",\n",
    "                           \"{:.3f}\".format(np.std(VAE_NB_score_cumm)), \"$ & \",\n",
    "                           \"$\",\"{:.3f}\".format(np.mean(VAE_LR_score_cumm)), \" \\pm  \",\n",
    "                           \"{:.3f}\".format(np.std(VAE_LR_score_cumm)), \"$\")\n",
    "\n",
    "print(\"_________________________GMVAE___________________________________\")\n",
    "print(\"$\",\"{:.3f}\".format(np.mean(GMV_RF_score_cumm)), \" \\pm  \",\n",
    "                           \"{:.3f}\".format(np.std(GMV_RF_score_cumm)), \"$ & \",\n",
    "                           \"$\",\"{:.3f}\".format(np.mean(GMV_SMO_score_cumm)), \" \\pm  \",\n",
    "                           \"{:.3f}\".format(np.std(GMV_SMO_score_cumm)), \"$ & \", \n",
    "                           \"$\",\"{:.3f}\".format(np.mean(GMV_NB_score_cumm)), \" \\pm  \",\n",
    "                           \"{:.3f}\".format(np.std(GMV_NB_score_cumm)), \"$ & \",\n",
    "                           \"$\",\"{:.3f}\".format(np.mean(GMV_LR_score_cumm)), \" \\pm  \",\n",
    "                           \"{:.3f}\".format(np.std(GMV_LR_score_cumm)), \"$  \")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"training score\")\n",
    "print(np.mean(RF_training_score), np.std(RF_training_score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
